# âš¡ LiteRAG â€” Local RAG Pipeline, Production-Ready from Your Own Hardware

> ğŸ§  **Based on:** [HandsOnLLMs (by silvaxxx1)](https://github.com/silvaxxx1/HandsOnLLMs)
> ğŸ¯ **Goal:** Transform the original Local RAG prototype into a **scalable, user-ready, robust RAG pipeline** â€” no cloud, no hassle, no expertise required.

---

## ğŸ”„ From HandsOnLLMs â†’ LiteRAG

This project is **not a fork of someone else's idea** â€” it's a **continuation and evolution of my own work** from the [HandsOnLLMs](https://github.com/silvaxxx1/HandsOnLLMs) repo.

In that repo, the `Local RAG/` subproject introduced a lightweight RAG system for local experimentation. **LiteRAG now productizes and scales that idea** by:

* Adding a full GUI for end users (Gradio / Streamlit)
* Creating auto-installers and Docker support
* Improving modularity and configurability
* Optimizing for performance on local hardware (CPU and GPU)
* Preserving CLI pipelines for developers and contributors

---

## ğŸ§­ What Is LiteRAG?

A **modular Retrieval-Augmented Generation (RAG)** pipeline that runs entirely offline â€” designed for **scalability, reliability, and ease of use**.

* ğŸ§© Modular architecture (easy to swap embedding or LLMs)
* ğŸ’» 100% local execution (no cloud, no vendor lock-in)
* ğŸ§  Small but capable models (like TinyLlama) for real-time CPU inference
* ğŸ“Š Efficient FAISS-based retrieval
* ğŸ–¥ï¸ Gradio UI for anyone to use â€” no code required

---

## ğŸ“¦ Folder Structure

```bash
LiteRAG/
â”œâ”€â”€ app.py                   # Main GUI entrypoint (for end users)
â”œâ”€â”€ installer/               # Auto-start scripts for Windows, macOS, Linux
â”œâ”€â”€ Dockerfile               # Containerized setup
â”œâ”€â”€ models/                  # Local LLMs (GGUF)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ assets/              # PDFs, chunked text, vector files
â”‚   â”œâ”€â”€ config/              # YAML configurations
â”‚   â”œâ”€â”€ data/                # Document loaders & chunkers
â”‚   â”œâ”€â”€ embedding/           # Embedding model logic
â”‚   â”œâ”€â”€ inference/           # Local LLM inference (llama.cpp, etc.)
â”‚   â”œâ”€â”€ retrieval/           # Vector search (FAISS)
â”‚   â”œâ”€â”€ utils/               # File I/O, logging, helpers
â”‚   â””â”€â”€ rag_pipeline.py      # CLI pipeline for developers
â””â”€â”€ README.md
```

---

## ğŸ–¥ï¸ How Users Interact (Simple, No Code)

End users donâ€™t need to touch the terminal.

> âœ… Just run one script:

```bash
bash installer/start.sh
```

Then:

* Upload your documents (PDF, TXT, etc.)
* Ask questions in chat
* All answers are generated **locally** using embedded knowledge from your own files

---

## âš™ï¸ How Developers Interact (Powerful, Modular CLI)

All pipeline stages are still available for advanced users:

```bash
# Chunk & preprocess docs
python src/rag_pipeline.py data --url http://example.com/file.pdf

# Embed with a specific model
python src/rag_pipeline.py embed --model-key sentence-transformers/all-MiniLM-L6-v2

# Run TinyLlama inference
python src/rag_pipeline.py inf --device cpu

# Full end-to-end RAG pipeline
python src/rag_pipeline.py all --url ... --chunk-size 512 --model-key ... --device cpu
```

> These CLI components are **preserved from the original HandsOnLLMs design**, now made more robust and configurable.

---

## ğŸ”§ Tech Stack

| Component     | Tool / Model                                    |
| ------------- | ----------------------------------------------- |
| Embeddings    | Sentence Transformers (MiniLM, BGE, Instructor) |
| Vector Search | FAISS (default), easily swappable               |
| LLM Inference | TinyLlama (GGUF) + llama.cpp                    |
| Runtime       | `llama-cpp-python`, `transformers`, YAML-based  |
| UI            | Gradio (or Streamlit, user-selectable)          |
| Deployment    | Bash scripts, Docker, cross-platform support    |

---

## ğŸ“ˆ Roadmap

| Status | Feature                                       |
| ------ | --------------------------------------------- |
| âœ…      | Modular local pipeline (data, embedding, LLM) |
| âœ…      | Developer CLI preserved from HandsOnLLMs      |
| âœ…      | Quantized TinyLlama inference via llama.cpp   |
| âœ…      | Gradio-based GUI for end users                |
| ğŸ”œ     | Windows/macOS GUI installers                  |
| ğŸ”œ     | GPU support fallback                          |
| ğŸ”œ     | Multi-format document support                 |
| ğŸ”œ     | Vector DB backends: ChromaDB, Qdrant          |
| ğŸ”œ     | Settings UI, Model/Embedding switcher         |

---

## ğŸ™ Credits & License

* Original repo: [HandsOnLLMs](https://github.com/silvaxxx1/HandsOnLLMs)
* License: MIT Â© 2025 [@silvaxxx1](https://github.com/silvaxxx1)
* Thanks to the open-source community for tools like `sentence-transformers`, `FAISS`, `llama.cpp`, and `TinyLlama`.

---

## ğŸ’¬ TL;DR

**LiteRAG** = hands-on meets hands-off.

* For developers: full control via CLI
* For users: one-click document QA chat
* For everyone: free, local, fast

---
